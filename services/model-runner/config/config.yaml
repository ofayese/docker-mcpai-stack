# Default model runner configuration
models:
  # Default models to load on startup
  - name: "smollm2-1.7b"
    path: "/models/smollm2-1.7b.gguf"
    type: "llama"
    context_length: 2048
    gpu_layers: 0
    
  - name: "llama3-8b"
    path: "/models/llama3-8b.gguf"
    type: "llama"
    context_length: 4096
    gpu_layers: 32

server:
  host: "0.0.0.0"
  port: 8080
  cors_origins: ["*"]
  max_concurrent_requests: 10

llama_cpp:
  n_ctx: 4096
  n_threads: 4
  use_mmap: true
  use_mlock: false
  verbose: false
